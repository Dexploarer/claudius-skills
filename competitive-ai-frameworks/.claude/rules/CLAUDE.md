# Competitive AI Frameworks - Claude Code Configuration

> **Multi-Agent Competitive Simulation Framework**
> Three specialized AI teams compete using reinforcement learning to adapt strategies

**Format:** All configurations follow the standard Claudius Skills template format with proper frontmatter, structured sections, and comprehensive documentation.

---

## ğŸ¯ Framework Overview

This directory contains three competitive AI frameworks:

1. **Bug Hunting Championship** - Security vulnerability detection
2. **Code Quality Championship** - Code improvement competition
3. **User Flow Olympics** - User experience optimization

Each framework:
- Uses 3 competing AI teams with different strategies
- Implements reinforcement learning for strategy adaptation
- Scores performance based on results and quality
- Declares winners and extracts best practices

---

## ğŸ—ï¸ Architecture

### Components

**1. Subagents** (`.claude/subagents/`)
- Specialized AI team members
- Each has unique strategy and focus areas
- Compete independently in each round

**2. Skills** (`.claude/skills/`)
- Automatic activation based on context
- Orchestrate full championship simulations
- Present results and recommendations

**3. Slash Commands** (`.claude/commands/`)
- Manual invocation of championships
- Quick access to specific frameworks
- Configurable parameters

**4. Python Frameworks** (`frameworks/`)
- Coordinators for each championship
- Scoring engines for performance evaluation
- Reinforcement learning for adaptation
- Metrics tracking across rounds

---

## ğŸª Available Frameworks

### 1. Bug Hunting Championship

**Purpose:** Find security vulnerabilities competitively

**Teams:**
- Team 1: Automated Scanners (pattern matching, static analysis)
- Team 2: Manual Reviewers (business logic, auth/authz)
- Team 3: Fuzzers (race conditions, edge cases)

**Activation:**
- Skill: Mention "find vulnerabilities", "security audit"
- Command: `/run-bug-hunt --target <path>`

**Scoring:**
- CVSS-based severity (Critical=100, High=50, Medium=25, Low=10)
- Uniqueness bonus (+50%)
- Quality bonus (+0-20 points)
- False positive penalty (-20 points)

### 2. Code Quality Championship

**Purpose:** Improve code quality metrics competitively

**Teams:**
- Team 1: Performance Optimizers (runtime, memory, bundle size)
- Team 2: Maintainability Engineers (complexity, documentation)
- Team 3: Best Practices Auditors (style, tests, accessibility)

**Activation:**
- Skill: Mention "improve code quality", "refactor"
- Command: `/run-quality-check --target <path>`

**Scoring:**
- Metrics improvement percentage
- Number of issues resolved
- Impact on overall quality score
- Regression prevention

### 3. User Flow Olympics

**Purpose:** Optimize user experience flows competitively

**Teams:**
- Team 1: Happy Path Optimizers (core flows, conversion)
- Team 2: Edge Case Handlers (error states, accessibility)
- Team 3: Integration Specialists (cross-system flows)

**Activation:**
- Skill: Mention "test user flows", "UX testing"
- Command: `/run-flow-test --flows <list>`

**Scoring:**
- Flow completion success rate
- User experience metrics
- Error handling coverage
- Performance benchmarks

---

## ğŸ§  Reinforcement Learning

### How It Works

Each team maintains strategy weights that adapt based on performance:

```python
new_weight = old_weight + learning_rate * (reward - baseline) * frequency

# Where:
# - reward: Points earned for this vulnerability/issue type
# - baseline: Historical average for this type
# - frequency: How often this type was found
# - learning_rate: 0.15 (how quickly to adapt)
```

### Adaptation Cycle

```
Round N:
1. Teams execute with current strategy weights
2. Scoring engine evaluates results
3. Reinforcement algorithm updates weights
4. Successful patterns strengthened
5. Failed approaches deprioritized

Round N+1:
6. Teams use adapted strategies
7. Repeat cycle with improved tactics
```

### Benefits

- **Automatic optimization:** Strategies improve over time
- **Competitive evolution:** Teams adapt to competition
- **Pattern recognition:** Successful techniques amplified
- **Mistake learning:** False positives reduce over rounds

---

## ğŸ® Usage Guide

### Quick Start

**Run Bug Hunting:**
```bash
/run-bug-hunt --target ./src --rounds 5
```

**Run Code Quality:**
```bash
/run-quality-check --target ./src --rounds 3
```

**Run User Flow Testing:**
```bash
/run-flow-test --flows registration,checkout,profile
```

### Automatic Activation

Skills activate automatically on relevant mentions:

```
User: "I need to find security vulnerabilities in this codebase"
â†’ bug-hunting-simulator skill activates

User: "Help me improve the code quality"
â†’ code-quality-analyzer skill activates

User: "Test all user flows"
â†’ user-flow-tester skill activates
```

### Advanced Configuration

**Focus on specific team:**
```bash
/run-bug-hunt --target ./src --team manual
```

**More rounds for better adaptation:**
```bash
/run-quality-check --target ./src --rounds 10
```

**With visualization:**
```bash
/run-bug-hunt --target ./src --visualize
```

---

## ğŸ“Š Understanding Results

### Championship Output

Each championship provides:

1. **Final Standings**
   - Winner with total score
   - Runner-up teams
   - Performance breakdown

2. **Top Discoveries**
   - Best findings from all teams
   - Severity/impact ratings
   - Remediation guidance

3. **Strategy Analysis**
   - Why the winner succeeded
   - Key techniques used
   - Performance metrics

4. **Combined Recommendations**
   - Best practices from all teams
   - Optimal approach synthesis
   - Implementation guidance

### Example Output

```
ğŸ¥‡ WINNER: Team 2 - Manual Reviewers
   Score: 1,450 points
   Critical Bugs: 2
   False Positive Rate: 5%

WINNING STRATEGY:
- Focused on business logic flaws
- Deep authentication analysis
- High-quality vulnerability reports

RECOMMENDED APPROACH:
1. Start with automated scanning (Team 1)
2. Follow with manual review (Team 2)
3. Apply fuzzing to critical areas (Team 3)
```

---

## ğŸ” Security & Ethics

### Authorized Use Only

**Always confirm:**
- âœ… User has authorization to test
- âœ… Defensive security or educational purpose
- âœ… User's own code or authorized environment

**Never assist with:**
- âŒ Unauthorized penetration testing
- âŒ Malicious exploitation
- âŒ Production system attacks without authorization
- âŒ Evasion of security controls

### Built-in Safety

- Authorization confirmation required
- Read-only analysis by default
- No automatic exploitation
- All activities logged

---

## ğŸ“ Directory Structure

```
competitive-ai-frameworks/
â”œâ”€â”€ .claude/
â”‚   â”œâ”€â”€ skills/                    # Automatic capabilities
â”‚   â”‚   â”œâ”€â”€ bug-hunting-simulator.md
â”‚   â”‚   â”œâ”€â”€ code-quality-analyzer.md
â”‚   â”‚   â””â”€â”€ user-flow-tester.md
â”‚   â”œâ”€â”€ commands/                  # Manual shortcuts
â”‚   â”‚   â”œâ”€â”€ run-bug-hunt.md
â”‚   â”‚   â”œâ”€â”€ run-quality-check.md
â”‚   â”‚   â””â”€â”€ run-flow-test.md
â”‚   â”œâ”€â”€ subagents/                 # Team specialists
â”‚   â”‚   â”œâ”€â”€ team1-automated-scanner.md
â”‚   â”‚   â”œâ”€â”€ team2-manual-reviewer.md
â”‚   â”‚   â”œâ”€â”€ team3-fuzzer.md
â”‚   â”‚   â””â”€â”€ [9 more team configs]
â”‚   â”œâ”€â”€ hooks/                     # Event automation
â”‚   â”‚   â””â”€â”€ scoring-tracker.json
â”‚   â””â”€â”€ rules/                     # Framework rules
â”‚       â””â”€â”€ CLAUDE.md (this file)
â”œâ”€â”€ frameworks/                    # Core implementations
â”‚   â”œâ”€â”€ bug-hunting/
â”‚   â”‚   â”œâ”€â”€ coordinator.py
â”‚   â”‚   â”œâ”€â”€ scoring_engine.py
â”‚   â”‚   â”œâ”€â”€ metrics.py
â”‚   â”‚   â””â”€â”€ reinforcement.py
â”‚   â”œâ”€â”€ code-quality/
â”‚   â””â”€â”€ user-flows/
â”œâ”€â”€ examples/                      # Example targets
â”œâ”€â”€ docs/                          # Documentation
â””â”€â”€ README.md                      # Main guide
```

---

## ğŸ“ Educational Value

### Learn About

**Multi-Agent Systems:**
- Competitive AI coordination
- Strategy differentiation
- Performance comparison

**Reinforcement Learning:**
- Policy gradient methods
- Reward-based adaptation
- Exploration vs exploitation

**Security Testing:**
- Vulnerability detection methodologies
- Bug bounty scoring (CVSS)
- Responsible disclosure

**Code Quality:**
- Quality metrics (complexity, coverage)
- Best practices enforcement
- Technical debt reduction

---

## ğŸš€ Extending Frameworks

### Add New Team

1. Create subagent config in `.claude/subagents/team4-<name>.md`
2. Define strategy and focus areas
3. Update coordinator to include new team
4. Test and validate

### Customize Scoring

Edit `frameworks/*/scoring_engine.py`:

```python
SEVERITY_SCORES = {
    'critical': 150,  # Increase critical weight
    'high': 75,
    ...
}
```

### Adjust Learning Rate

Edit `frameworks/*/reinforcement.py`:

```python
LEARNING_RATE = 0.2  # Faster adaptation
EXPLORATION_RATE = 0.15  # More exploration
```

---

## ğŸ› Troubleshooting

**Issue:** Too many false positives
**Solution:** Increase validation, raise false positive penalty

**Issue:** Not finding critical bugs
**Solution:** Increase manual review team weight, add more rounds

**Issue:** Simulation too slow
**Solution:** Reduce rounds, focus on specific team

**Issue:** Strategies not improving
**Solution:** Increase exploration rate, verify reward signals

---

## ğŸ“š Documentation

Full documentation:
- `README.md` - Complete project guide
- `docs/bug-hunting-guide.md` - Bug hunting details
- `docs/scoring-system.md` - Scoring methodology
- `docs/reinforcement-learning.md` - RL algorithm details
- `docs/extending-frameworks.md` - Customization guide

---

## ğŸ¯ Success Metrics

Frameworks succeed when:
- All rounds complete successfully
- Clear winner determined
- Actionable recommendations provided
- Strategies demonstrably improve over rounds
- User understands results and next steps

---

## ğŸ’¡ Tips for Best Results

1. **Run multiple rounds** (5-10) for strategy adaptation
2. **Use visualization** to see improvement over time
3. **Focus teams** on their strengths
4. **Combine approaches** for optimal coverage
5. **Iterate** - run again after fixes to verify

---

## ğŸ”— Integration

These frameworks integrate with:

**Claude Code Tools:**
- `Grep` - Pattern matching and searching
- `Read` - Code file analysis
- `Bash` - Running tests and tools
- `Edit` - Applying fixes (if requested)

**External Tools:**
- Static analysis tools (bandit, eslint, etc.)
- Testing frameworks (pytest, jest, etc.)
- Security scanners (if available)
- Performance profilers

---

**Built with Claude Code's Five Pillars of Extensibility**
**Part of the Claudius Skills Project**
**Educational â€¢ Defensive Security â€¢ Code Quality**
